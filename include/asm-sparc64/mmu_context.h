multiline_comment|/* $Id: mmu_context.h,v 1.45 2000/08/12 13:25:52 davem Exp $ */
macro_line|#ifndef __SPARC64_MMU_CONTEXT_H
DECL|macro|__SPARC64_MMU_CONTEXT_H
mdefine_line|#define __SPARC64_MMU_CONTEXT_H
multiline_comment|/* Derived heavily from Linus&squot;s Alpha/AXP ASN code... */
macro_line|#ifndef __ASSEMBLY__
macro_line|#include &lt;linux/spinlock.h&gt;
macro_line|#include &lt;asm/system.h&gt;
macro_line|#include &lt;asm/spitfire.h&gt;
DECL|function|enter_lazy_tlb
r_static
r_inline
r_void
id|enter_lazy_tlb
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
comma
r_struct
id|task_struct
op_star
id|tsk
comma
r_int
id|cpu
)paren
(brace
)brace
r_extern
id|spinlock_t
id|ctx_alloc_lock
suffix:semicolon
r_extern
r_int
r_int
id|tlb_context_cache
suffix:semicolon
r_extern
r_int
r_int
id|mmu_context_bmap
(braket
)braket
suffix:semicolon
DECL|macro|CTX_VERSION_SHIFT
mdefine_line|#define CTX_VERSION_SHIFT&t;(PAGE_SHIFT - 3)
DECL|macro|CTX_VERSION_MASK
mdefine_line|#define CTX_VERSION_MASK&t;((~0UL) &lt;&lt; CTX_VERSION_SHIFT)
DECL|macro|CTX_FIRST_VERSION
mdefine_line|#define CTX_FIRST_VERSION&t;((1UL &lt;&lt; CTX_VERSION_SHIFT) + 1UL)
DECL|macro|CTX_VALID
mdefine_line|#define CTX_VALID(__ctx)&t;&bslash;&n;&t; (!(((__ctx) ^ tlb_context_cache) &amp; CTX_VERSION_MASK))
DECL|macro|CTX_HWBITS
mdefine_line|#define CTX_HWBITS(__ctx)&t;((__ctx) &amp; ~CTX_VERSION_MASK)
r_extern
r_void
id|get_new_mmu_context
c_func
(paren
r_struct
id|mm_struct
op_star
id|mm
)paren
suffix:semicolon
multiline_comment|/* Initialize a new mmu context.  This is invoked when a new&n; * address space instance (unique or shared) is instantiated.&n; * This just needs to set mm-&gt;context to an invalid context.&n; */
DECL|macro|init_new_context
mdefine_line|#define init_new_context(__tsk, __mm)&t;(((__mm)-&gt;context = 0UL), 0)
multiline_comment|/* Destroy a dead context.  This occurs when mmput drops the&n; * mm_users count to zero, the mmaps have been released, and&n; * all the page tables have been flushed.  Our job is to destroy&n; * any remaining processor-specific state, and in the sparc64&n; * case this just means freeing up the mmu context ID held by&n; * this task if valid.&n; */
DECL|macro|destroy_context
mdefine_line|#define destroy_context(__mm)&t;&t;&t;&t;&t;&bslash;&n;do {&t;spin_lock(&amp;ctx_alloc_lock);&t;&t;&t;&t;&bslash;&n;&t;if (CTX_VALID((__mm)-&gt;context)) {&t;&t;&t;&bslash;&n;&t;&t;unsigned long nr = CTX_HWBITS((__mm)-&gt;context);&t;&bslash;&n;&t;&t;mmu_context_bmap[nr&gt;&gt;6] &amp;= ~(1UL &lt;&lt; (nr &amp; 63));&t;&bslash;&n;&t;}&t;&t;&t;&t;&t;&t;&t;&bslash;&n;&t;spin_unlock(&amp;ctx_alloc_lock);&t;&t;&t;&t;&bslash;&n;} while(0)
multiline_comment|/* Reload the two core values used by TLB miss handler&n; * processing on sparc64.  They are:&n; * 1) The physical address of mm-&gt;pgd, when full page&n; *    table walks are necessary, this is where the&n; *    search begins.&n; * 2) A &quot;PGD cache&quot;.  For 32-bit tasks only pgd[0] is&n; *    ever used since that maps the entire low 4GB&n; *    completely.  To speed up TLB miss processing we&n; *    make this value available to the handlers.  This&n; *    decreases the amount of memory traffic incurred.&n; */
DECL|macro|reload_tlbmiss_state
mdefine_line|#define reload_tlbmiss_state(__tsk, __mm) &bslash;&n;do { &bslash;&n;&t;register unsigned long paddr asm(&quot;o5&quot;); &bslash;&n;&t;register unsigned long pgd_cache asm(&quot;o4&quot;); &bslash;&n;&t;paddr = __pa((__mm)-&gt;pgd); &bslash;&n;&t;pgd_cache = 0UL; &bslash;&n;&t;if ((__tsk)-&gt;thread.flags &amp; SPARC_FLAG_32BIT) &bslash;&n;&t;&t;pgd_cache = pgd_val((__mm)-&gt;pgd[0]) &lt;&lt; 11UL; &bslash;&n;&t;__asm__ __volatile__(&quot;wrpr&t;%%g0, 0x494, %%pstate&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;mov&t;%3, %%g4&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;mov&t;%0, %%g7&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;stxa&t;%1, [%%g4] %2&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;wrpr&t;%%g0, 0x096, %%pstate&quot; &bslash;&n;&t;&t;&t;     : /* no outputs */ &bslash;&n;&t;&t;&t;     : &quot;r&quot; (paddr), &quot;r&quot; (pgd_cache),&bslash;&n;&t;&t;&t;       &quot;i&quot; (ASI_DMMU), &quot;i&quot; (TSB_REG)); &bslash;&n;} while(0)
multiline_comment|/* Set MMU context in the actual hardware. */
DECL|macro|load_secondary_context
mdefine_line|#define load_secondary_context(__mm) &bslash;&n;&t;__asm__ __volatile__(&quot;stxa&t;%0, [%1] %2&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;flush&t;%%g6&quot; &bslash;&n;&t;&t;&t;     : /* No outputs */ &bslash;&n;&t;&t;&t;     : &quot;r&quot; (CTX_HWBITS((__mm)-&gt;context)), &bslash;&n;&t;&t;&t;       &quot;r&quot; (0x10), &quot;i&quot; (0x58))
multiline_comment|/* Clean out potential stale TLB entries due to previous&n; * users of this TLB context.  We flush TLB contexts&n; * lazily on sparc64.&n; */
DECL|macro|clean_secondary_context
mdefine_line|#define clean_secondary_context() &bslash;&n;&t;__asm__ __volatile__(&quot;stxa&t;%%g0, [%0] %1&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;stxa&t;%%g0, [%0] %2&bslash;n&bslash;t&quot; &bslash;&n;&t;&t;&t;     &quot;flush&t;%%g6&quot; &bslash;&n;&t;&t;&t;     : /* No outputs */ &bslash;&n;&t;&t;&t;     : &quot;r&quot; (0x50), &quot;i&quot; (0x5f), &quot;i&quot; (0x57))
multiline_comment|/* Switch the current MM context. */
DECL|function|switch_mm
r_static
r_inline
r_void
id|switch_mm
c_func
(paren
r_struct
id|mm_struct
op_star
id|old_mm
comma
r_struct
id|mm_struct
op_star
id|mm
comma
r_struct
id|task_struct
op_star
id|tsk
comma
r_int
id|cpu
)paren
(brace
r_int
r_int
id|ctx_valid
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|mm-&gt;page_table_lock
)paren
suffix:semicolon
r_if
c_cond
(paren
id|CTX_VALID
c_func
(paren
id|mm-&gt;context
)paren
)paren
id|ctx_valid
op_assign
l_int|1
suffix:semicolon
r_else
id|ctx_valid
op_assign
l_int|0
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|ctx_valid
op_logical_or
(paren
id|old_mm
op_ne
id|mm
)paren
)paren
(brace
r_if
c_cond
(paren
op_logical_neg
id|ctx_valid
)paren
id|get_new_mmu_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|load_secondary_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|reload_tlbmiss_state
c_func
(paren
id|tsk
comma
id|mm
)paren
suffix:semicolon
)brace
(brace
r_int
r_int
id|vm_mask
op_assign
(paren
l_int|1UL
op_lshift
id|cpu
)paren
suffix:semicolon
multiline_comment|/* Even if (mm == old_mm) we _must_ check&n;&t;&t; * the cpu_vm_mask.  If we do not we could&n;&t;&t; * corrupt the TLB state because of how&n;&t;&t; * smp_flush_tlb_{page,range,mm} on sparc64&n;&t;&t; * and lazy tlb switches work. -DaveM&n;&t;&t; */
r_if
c_cond
(paren
op_logical_neg
id|ctx_valid
op_logical_or
op_logical_neg
(paren
id|mm-&gt;cpu_vm_mask
op_amp
id|vm_mask
)paren
)paren
(brace
id|mm-&gt;cpu_vm_mask
op_or_assign
id|vm_mask
suffix:semicolon
id|clean_secondary_context
c_func
(paren
)paren
suffix:semicolon
)brace
)brace
id|spin_unlock
c_func
(paren
op_amp
id|mm-&gt;page_table_lock
)paren
suffix:semicolon
)brace
multiline_comment|/* Activate a new MM instance for the current task. */
DECL|function|activate_mm
r_static
r_inline
r_void
id|activate_mm
c_func
(paren
r_struct
id|mm_struct
op_star
id|active_mm
comma
r_struct
id|mm_struct
op_star
id|mm
)paren
(brace
r_int
r_int
id|vm_mask
suffix:semicolon
id|spin_lock
c_func
(paren
op_amp
id|mm-&gt;page_table_lock
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
id|CTX_VALID
c_func
(paren
id|mm-&gt;context
)paren
)paren
id|get_new_mmu_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|vm_mask
op_assign
(paren
l_int|1UL
op_lshift
id|smp_processor_id
c_func
(paren
)paren
)paren
suffix:semicolon
r_if
c_cond
(paren
op_logical_neg
(paren
id|mm-&gt;cpu_vm_mask
op_amp
id|vm_mask
)paren
)paren
id|mm-&gt;cpu_vm_mask
op_or_assign
id|vm_mask
suffix:semicolon
id|spin_unlock
c_func
(paren
op_amp
id|mm-&gt;page_table_lock
)paren
suffix:semicolon
id|load_secondary_context
c_func
(paren
id|mm
)paren
suffix:semicolon
id|clean_secondary_context
c_func
(paren
)paren
suffix:semicolon
id|reload_tlbmiss_state
c_func
(paren
id|current
comma
id|mm
)paren
suffix:semicolon
)brace
macro_line|#endif /* !(__ASSEMBLY__) */
macro_line|#endif /* !(__SPARC64_MMU_CONTEXT_H) */
eof
